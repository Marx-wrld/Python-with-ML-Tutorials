{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning & Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reinforcement learning- science of learning by interacting with the environment.\n",
    "\n",
    "###Reasons for learning it\n",
    "###.Find previously unknown solutions.e.g- program that can play go better than any human, ever\n",
    "###.Find solutions online, for unforseen circumstances.e.g- a robot that can navigate terrains that differ greatly from any expected terrain.\n",
    "###.Rl-is able to find algorithms for both these cases.\n",
    "\n",
    "###It requires us to think about-\n",
    "###-time\n",
    "###-(long term)consequences of actions\n",
    "###-actively gathering experience\n",
    "###-predicting the future\n",
    "###-dealing with uncertainity\n",
    "\n",
    "###How does reinforcement learning differ from other machine learning paradigms\n",
    "###-no supervision,only a reward signal\n",
    "###-Feedback can be delayed, not instantaneous\n",
    "###-Time matters\n",
    "###-Earlier decisons affect later interactions \n",
    "###Examples\n",
    "###-fly a helicopter\n",
    "###-manage an investment portfolio\n",
    "###-control a power station\n",
    "###-make a robot walk\n",
    "###-play a video or board game \n",
    "\n",
    "###core concepts of a reinforcement learning system are:\n",
    "###-environment \n",
    "###-reward signal\n",
    "###-agent containing:-\n",
    "###.agent state\n",
    "###.policy\n",
    "###.value function\n",
    "###.model\n",
    "\n",
    "###Agent and Environment\n",
    "###-At each step t the agent:\n",
    "###.Receives observation Ot(and reward Rt)\n",
    "###.Executes action At\n",
    "###-The environment\n",
    "###.Receives action At\n",
    "###.Emits observation Ot+1(and reward Rt+1)\n",
    "\n",
    "###Rewards\n",
    "###-a reward Rt is a scalar feedback signal/reinforcement signal\n",
    "###-indicates how well agent is doing at step t -defines the goal\n",
    "###-The agent's job is to maximize cumulative reward/reward over time\n",
    "###Gt = Rt+1 + Rt+2 + Rt+3 + ...\n",
    "###we call this the return /Expected return \n",
    "###Reinforcement learning is based on the reward hypothesis\n",
    "\n",
    "###Values \n",
    "###We call the expected(expectation) cumulative reward, from a state s, the value\n",
    "###v(s) = E[Gt| St = s]\n",
    "### = E[Rt+1 + Rt+2 + Rt+3 + ...|St = s]\n",
    "###Goal is then to maximize value, by picking suitable actions \n",
    "###Rewards and values define desirability of a state or action (no supervised feedback)\n",
    "###Note that returns and values can be defined recursively \n",
    "###- Gt = Rt+1 + Gt+1\n",
    "\n",
    "###Actions in sequential problems \n",
    "###-Goal: Select actions to maximise value \n",
    "###Actions may have long term consequences \n",
    "###Reward may have long term consequences\n",
    "###Reward may be delayed\n",
    "###It may be better to sacrifice immediate reward to gain more long term reward \n",
    "\n",
    "###Examples:\n",
    "###-A financial investment (may take months to mature)\n",
    "###-Refueling a helicopter (might prevent a crash in several hours)\n",
    "###-Blocking opponent moves(might help winning chances many moves from now)\n",
    "###A mapping from states to actions is called a policy\n",
    "###It is possible to condition the value on actions:\n",
    "###- q(s,a) = E[Gt | St = s,At = a]\n",
    "###-        = E[Rt+1 + Rt+2 + Rt+3 + ...| St = s,At = a]\n",
    "###NOTE-for state we use (v) for state action we use (q)\n",
    "\n",
    "###State\n",
    "###Actions depend on the state of the agent\n",
    "###Both agent and environment may have an internal state\n",
    "###In the simplest case, there is only one state\n",
    "###Often, there are many different states -sometimes infinitely many\n",
    "###The state of the agent generally differs from the state of the environment\n",
    "###The agent may not even know the full state of the environment\n",
    "###The environment state is the environment's internal state\n",
    "###it is not usuallly visible to the agent \n",
    "###Even if it is visible, it may contain lots of irrelevant information \n",
    "\n",
    "###Agent state\n",
    "###A history is a sequence of observations, actions, rewards \n",
    "### Ht = O0, A0, R1, O1,...,Ot-1,At-1,Rt,Ot\n",
    "###For instance the sensorimotor stream of a robot\n",
    "###This history can be used to construct an agent state St\n",
    "###Actions depend on this state\n",
    "\n",
    "###Full Observability \n",
    "###Suppose the agent sees the full environment state\n",
    "###Observation = Environment state\n",
    "###The agent state could just be this observation:\n",
    "### St = Ot = Environment state\n",
    "### Then the agent is in a Markov decision process\n",
    "\n",
    "###Markov decision processes\n",
    "###(MDPs)provide a useful mathematical framework\n",
    "###A decison process is Markov if\n",
    "###- p (r,s | St,At) = p(r,s | Ht,At)\n",
    "###-if the probability of a reward and subsequence state(written as a joint probability)\n",
    "###-the way they depend on your current state and action is \n",
    "###-fully informative if you would condition with the full history\n",
    "###This basically means that your current state gives you the whole information & ability to predict the next reward\n",
    "###and the next state\n",
    "###So if this probability is fixed even if the agent doesn't know if it exists its a markov decision process\n",
    "   \n",
    "###The future is independent of the past given the present\n",
    "### Ht -> St -> Ht+1\n",
    "###Once the state is known,the history may be thrown away(in practical)\n",
    "###The environment state is typically Markov\n",
    "###The history Ht is Markov.\n",
    "\n",
    "###Partial Observability\n",
    "###The agent gets partial information\n",
    "###-A robot with camera vision isn't told its absolute location\n",
    "###-A poker playing agent only observes public cards \n",
    "###Now the observation is not Markov\n",
    "###Formally this is a partially observable Markov decision process\n",
    "###The environment state can still be Markov, but the agent does not know\n",
    "\n",
    "###The Agent state \n",
    "###The agent state is a function of the history\n",
    "###For instance, St = Ot\n",
    "###You can think of the agent state as something that updates over time\n",
    "###More generally:\n",
    "### St+1 = f(St,At,Rt+1,Ot+1)\n",
    "### where f is a state update function\n",
    "### The agent state is typically much smaller than the environment state\n",
    "\n",
    "###Partially observable environments\n",
    "###To deal with partial observability, agent can construct suitable state representations\n",
    "###Examples of agent states:\n",
    "###-Last observation: St = Ot (might not be enough)\n",
    "###-Complete history: St = Ht (might be too large)\n",
    "###-Some incrementally updated state: St = f(St-1,Ot)\n",
    "###(E.g implemented with a recurrent neural network)\n",
    "###(Sometimes called memory)\n",
    "###-constructing a Markov agent state may not be feasible; this is common!\n",
    "###-More importantly, the state should be containing enough informative for good policies, and/ or good value predictions\n",
    "\n",
    "###Policy\n",
    "###A policy defines the agent's behaviour\n",
    "###It is a map from the agent's state to action\n",
    "###Two main cases\n",
    "###Deterministic policy: A = pi(S)-write it as an action ,state goes in... action goes out\n",
    "###Stochastic policy: pi(A|S) = p(A|S)-where their is a probability of selecting each action in each state\n",
    "\n",
    "###Value function\n",
    "###The actual value function is the expected return,conditional state,conditional policy \n",
    "### v[pi](S) = E[Gt | St = s,pi]\n",
    "###          = E[Rt+1 + rRt+2 + r(sqrd)Rt+3 + ...| St = s,pi]\n",
    "###We introduced a discount factor r E [0,1]\n",
    "###Gamma=1,(accumulation of your rewards into the future), in many occasions we actually pick a gamma that is less than one\n",
    "###- Trades off importance of immediate vs long term rewards putting higher weights on the immediate rewards\n",
    "###Basically your downweighing the future rewards in favour of the immediate ones\n",
    "###-If you think of the previous example where you get a zero reward while in the maze and a +1 reward when you exit the \n",
    "### maze.., if you don't have downweighing the agent doesn't have incentives to exit the maze quickly,it will just be happy \n",
    "### if it exits the maze sometime into the future.\n",
    "###-but when you have this,then the trader starts to differ and you take this trader which is a little faster and takes fewer\n",
    "### steps to exit the maze the discounts will become less\n",
    "###The value depends on a policy\n",
    "###Can be used to evaluate the desirability of states \n",
    "###Can be used to select between actions\n",
    "###The return has a recursive form  Gt = Rt+1 + rGt+1\n",
    "###Therefore the value has as well a recursive form\n",
    "### v(pi) = E[Rt+1 + rGt+1 | St = s,At ~ pi(s)]\n",
    "###       = E[Rt+1 + r v pi(St+1) | St = s,At ~ pi(s)]\n",
    "###Here a ~ pi(s) means a is chosen by policy pi in  state s(even if pi is deterministic)\n",
    "###This is known as Bellman equation(Bellman 1957)\n",
    "###A similar equation holds for the optimal (=highest possible) value:\n",
    "###     v.(s) = maxE[Rt+1 + rv.(St+1) | St = s,At = a]\n",
    "###               a\n",
    "###This does not depend on a policy\n",
    "###We heavily exploit such equalities and use them to create algorithms\n",
    "### This equation Takes the action that maximizes the one step and then uses the optimal value on the next step\n",
    " \n",
    "###Value function approximations\n",
    "###Agents often approximate value functions\n",
    "###We will discuss algorithms to learn these efficiently \n",
    "###With an accurate value function, we can behave optimally\n",
    "###with suitable approximations, we can behave well, even in intractably big domains\n",
    "\n",
    "###Model-prediction of what the environment dynamics are.\n",
    "### A model predicts what the environment will do next\n",
    "###E.g 'P' predicts the next state\n",
    "###   P(s,a,s') = p(St+1 = s'|St = s,At = a)\n",
    "###E.g 'R' predicts the next (immediate)reward\n",
    "###  R(s,a) = E[Rt+1 | St = s,At = a]\n",
    "###A model does not immediately give us a good policy- we would still need to plan\n",
    "### We would consider stochastic(generative) models\n",
    "\n",
    "###Categorizing agents- There are many different ways you can built an agent\n",
    "###-Value Based-Here the agent has some approximate value function that it uses to judge which actions are better than others there might not be an explicit policy in that case\n",
    "###.Value function\n",
    "###-Policy Based\n",
    "###.policy\n",
    "###-Actor Critic\n",
    "###.Policy\n",
    "###.Value function\n",
    "###There is also the categorizing based on the model free and the model based agent\n",
    "###-Model Free\n",
    "###.policy and/or value function\n",
    "###-Model Based \n",
    "###.Optionally Policy/or value function\n",
    "###.Model\n",
    "\n",
    "###Learning and Planning\n",
    "###Two fundamnetal problems in reinforcement learning\n",
    "###Learning\n",
    "###-The environment is initially unknown \n",
    "###-The agent interacts with the environment and therefore needs to come out with a better policy\n",
    "###Planning\n",
    "###A model of the environmnet is given/learnt\n",
    "###The agent plans in this model(without external interaction)\n",
    "###a.k.a. reasoning,pondering,thought,search,planning\n",
    "\n",
    "###Prediction and control\n",
    "###prediction : evaluate the future(for a given policy)\n",
    "###control : optimize the future( find the best policy)\n",
    "###These are strongly related :\n",
    "###                             pi.(s) = argmax v pi(s)\n",
    "###                                         pi\n",
    "###If we could predict everything do we need anything else?\n",
    "\n",
    "###All the components are functions \n",
    "###-Policies map states to actions \n",
    "###-Value functions map states to values \n",
    "###-Models map states to states and /or rewards \n",
    "###-State updates map states and observations to new states \n",
    "###We could represents these functions as neural networks, then use deep learning methods to optimize these\n",
    "###Take care: we often violate assumptions from supervised learning (iid,stationarity)\n",
    "###Deep reinforcement learning is a rich and active research field\n",
    "###(Current) neural networks are not always the best tool(but they often work well)\n",
    "\n",
    "###Atari example:Reinforcement learning\n",
    "###Rules of the game are unknown \n",
    "###Learn directly from interactive gameplay\n",
    "###Pick actions on joystick, see pixels and scores\n",
    "\n",
    "###Atari example:Planning\n",
    "###Rules of the game are known \n",
    "###Can query emulator:perfect model\n",
    "###If i take action a from state s:\n",
    "###-what would the next state be?\n",
    "###-what would the score be?\n",
    "###Plan ahead to find optimal policy \n",
    "###Later versions add noise, to break algorithms\n",
    "###that rely on determinism\n",
    "\n",
    "###Exploration and exploitation\n",
    "###We learn by trial and error \n",
    "###The agent should discover  a good policy\n",
    "###...from new experiences \n",
    "###...without sacrifycing too much reward along the way\n",
    "###Exploration finds more information \n",
    "###Exploitation exploits known inforamtion to maximize reward \n",
    "###It is important to explore as well as exploit \n",
    "###This is a fundamental problem that does not occur in supervised learning\n",
    "\n",
    "###Examples \n",
    "###Restaurant selection\n",
    "###-Exploitation- Go to your favourite restaurant\n",
    "###-Exploration-Try a new restaurant \n",
    "###Oil drilling\n",
    "###-Exploitation-Drill at the best known location\n",
    "###-Explorartion-Drill at a new location\n",
    "###Game playing\n",
    "###Exploitation-Play the move you currently believe is best\n",
    "###Exploration-Try a new strategy\n",
    "\n",
    "###Course\n",
    "###In this course, we discuss how to learn by interaction\n",
    "###The focus is on understanding core principles and learning algorithms\n",
    "###Topics include\n",
    "###Exploration, in bandits and in sequential problems\n",
    "###Markov decision processes, and planning by dynamic programming\n",
    "###Model free prediction and control(e.g.-Q-learning)\n",
    "###Policy-gradient methods \n",
    "###Challenges in deep reinforcement learning\n",
    "###Intergrating learning and planning\n",
    "###Guest lectures by Vlad and David Silver\n",
    "\n",
    "###second lecture \n",
    "\n",
    "###Reinforcement learning is the science of learning to make decisions when you want to optimize this reward signal/when you want to learn about the world or predict future observations\n",
    "###Agents can learn a policy, value function and or a model\n",
    "###The general problem involves taking into account time &consequences and \n",
    "###especially at this last point your actions may not just change your immediate reward but also the world(Later decisions are affected)\n",
    "###E.g-a simple change like you moving here to their\n",
    "###The general problem involves taking into account time and consequences\n",
    "###Decisions affect the reward, the agent state, and environment state\n",
    "\n",
    "###consider simple case:multiple actions,but only one state\n",
    "###No sequential structure - past actions do not influence the future state of the environment\n",
    "###Initially: You wil have the opportunity to interact with the problem again and again without changing the problem\n",
    "###Formally: the distribution of Rt(Reward) given At(Action) is identical and independent across time \n",
    "\n",
    "###Exploration vs Exploitation\n",
    "###Online decision-making involves a fundamental choice: \n",
    "###-Exploitation: Maximize performance based on current knowledge \n",
    "###-Exploration-Increase knowledge about the problem\n",
    "###The best long term strategy may involve short-term sacrifices \n",
    "###We want to gather enough information to make the best overall decisions.\n",
    "\n",
    "###The multi armed bandit formalised as the simplest setting in Analogy to the one armed bandit\n",
    "###Same as the swap machine in the casino where there is a lever that you pull then you either get some money or you don't \n",
    "###Its called the one armed bandit because in the long run its steals you some money\n",
    "###In the multi armed bandit think of a long wave of machines in which each decision corresponds to one of these machines\n",
    "###and they have different pay offs \n",
    "###Sometimes called the game against nature because your basically playing against the environment\n",
    "\n",
    "###Action values - something depended on the action there is no state your basically always on the same state\n",
    "###The true value of an action is justified by the expected reward of taking that action\n",
    "###We can estimate this simply by taking the average\n",
    "###Whenever you pick an action you just average that reward and then this will give you an estimate for the expected reward\n",
    "\n",
    "###Regret\n",
    "###How can we reason about the exploration trade off?\n",
    "###Seems natural to somehow take into account that estimates can be uncertain\n",
    "###Can we reason about this formally?\n",
    "###Can we trade off exploration and exploitation optimally?\n",
    "### V* -is not a function of anything, which is the maximum true action value\n",
    "###The opportunity loss for each step-Regret\n",
    "### V* - q(At) -actual value q, so this the true value of the action you selected At\n",
    "### In hindsight, I might 'regret' taking the tube rather than cycling\n",
    "###I might have regretted taking a bus even more\n",
    "###The agent cannot observe, or even sample, the real regret directly\n",
    "###But we can use it to analyze different learning algorithms\n",
    "###Goal - Trade-off exploration and exploitation by minimizing total regret \n",
    "###Maximize cumulative reward = minimise total regret \n",
    "###Note: the sum extends beyond (single step) episodes\n",
    "###View extends over \"lifetime of learning\" rather than over \"Current episode\".\n",
    "\n",
    "###Regret can grow unbounded \n",
    "###More interesting is how fast it grows \n",
    "###The greedy policy has linear regret\n",
    "###This means that, in expectation, the regret grows as a function that is linear in t(no. of steps that it takes)\n",
    "###Suppose p(cheese|white) = 0.1 and p(cheese|black) = 0.9\n",
    "###Then v* = q(black) =0.8 and q(white) = -0.8\n",
    "###The greedy rat incurs regret of 1.6t(if the first two actions and rewards are as shown)\n",
    "\n",
    "###Counting regret \n",
    "###The action regret(delta a) for a given action is the difference between the optimal value and the true value of a:\n",
    "###This delta is 0 for the optimal action and it will be positive for any action that is not optimal\n",
    "###Total regret then depends on action counts\n",
    "###A good algorithm ensures small counts for large action regets \n",
    "###But action regrets are not known..\n",
    "\n",
    "###Exploration\n",
    "###We need to explore to learn the values \n",
    "###One common solution:absolom -greedy\n",
    "###-Select greedy action (exploit) w.p 1-absolom\n",
    "###-Select random action (explore) w.p.absolom\n",
    "###Is this enough?\n",
    "###How to pick absolom?\n",
    "\n",
    "###Greedy algorithm\n",
    "###Greedy can lock onto a suboptimal action forever\n",
    "###-Greedy has linear expected total regret\n",
    "\n",
    "\n",
    "###lower Bound\n",
    "###The performance if any algorithm is determined by similarity between optimal arm and other arms\n",
    "###Hard problems have arms with simiilar distributions but different means \n",
    "###This is descibed formally by the gap (delta a) and the similarity in distributions KL(P(r|a)||p(r|a*))\n",
    "\n",
    "###Upper confidence bounds \n",
    "###Estimate an upper confidence Ut(a) for each action value \n",
    "###-The true value with the high probability is smaller than your current estimate plus that bonus \n",
    "###Such that q(a) </= Qt(a) + Ut(a) with high probability\n",
    "###Select action maximizing upper confidence bound(UCB)\n",
    "###The uncertainity depends on the number of times N(a) has been selected \n",
    "###-Small Nt(a)=>large Ut(a) (Estimated value is uncertain)\n",
    "###-Large Nt(a)=>small Ut(a) (Estimated value is accurate)\n",
    "###For averages the uncertainity decreases as sqrt of Nt(a) by the central limit theorem(if variance of rewards is bounded)\n",
    "###Can we derive an optimal algorithm?\n",
    "\n",
    "###Algorithm idea\n",
    "###Recall, we want to minimize the (total regret),number of times you have selected that action times how bad you selected that action.\n",
    "###If the gap is big then we want the no of times we select this action to be small\n",
    "###If the no of times we select this action is big then we want this gap to be small\n",
    " ###Select action with the low gap than the action with the more gap.\n",
    "\n",
    "###Hoeffding's inequality\n",
    "###-These are a more general family of results which are called concentration bounds- what it means is that we can say something about \n",
    "###how this estimate behaves without characterizing the whole distribution\n",
    "\n",
    "###Calculating upper confidence bounds\n",
    "###Lets say we want to pick the probability p that true value exceeds UCB\n",
    "\n",
    "###UCB\n",
    "###We have an estimate Q and we are going do add sth that is the squareroot of the log t(our time step) divided by the no of times you\n",
    "###selected this specific action.\n",
    "###This means that for this action let us consider an action that you have not selected in a long long time, this means that this bonus\n",
    "###will keep on growing because of the log t and if you never selected it for a long time the bonus wil grow and grow until it goes higher \n",
    "###than all the other estimates plus bonuses for all the other actions, at that point you will select it. When you select it the no of times you \n",
    "###selected this action will go up which means the bonus drops at the same time you got a new sample, so your Q(estimate) for this action will also change\n",
    "###it might go up or might go down.\n",
    "###What log t actually does is that it bubbles up all the action probabilities. Each of this actions basically will get selected indefinitely again\n",
    "###because this bound keeps on growing but whenever you select it you kind of whack it back down again and only if your actual estimate is high will you continue \n",
    "###to select this action.\n",
    "###When will you select an action,(1)Either when uncertain  N is small(relative to the other actions) or when the estimated value is high\n",
    "###One factor to consider is that c reps 95% confidence or a 99% confidence\n",
    "###The c just regulates how quickly you learn\n",
    "\n",
    "###Values or Models?\n",
    "###learning a model-we could have a reward model that basically predicts the reward for each action but that seems very similar to the value-based algorithm in this case\n",
    "###This might be indistinguishable in some sense.You could interpret the action value estimates as a reward model because they are basically a prediction of what the expected \n",
    "###reward is when you select an action\n",
    "\n",
    "###Bayesian bandits\n",
    "##Bayesian bandits model parameterize distributions over rewards, p(Rt|0,a)\n",
    "###Compute posterior distribution over 0\n",
    "###Allow us to inject rich prior knowledge p0(0|a)\n",
    "###Use posterior to guide exploration\n",
    "###-Upper confidence bounds\n",
    "###-Probability matching\n",
    "###If you use bayes rule to update this distributions basically starting from a prior p0 we could update the model parameters each time we see a reward vs an action\n",
    "\n",
    "###Bayesian Bandits: Example\n",
    "###Consider bandits with Bernoulli reward distribution: rewards are 0 or +1 with an unknown probability\n",
    "###The expected reward here is just the probability that its 1\n",
    "###We want to basically model a belief over this probability\n",
    "###For each action, the prior could be a uniform distribution on [0,1]\n",
    "###This means we think each mean reward in [0,1] is equally likely\n",
    "###left 1-all of this true values are equally likely\n",
    "\n",
    "###Probability matching\n",
    "###we select an action according to the probability that it is optimal\n",
    "###this basically means that we are defining a policy and we are going to make this policy equal to the probability that the true value is the maximum true value under the history here\n",
    "\n",
    "###Thompson sampling\n",
    "###Which means that basically we are going to sample from each of these probabilities\n",
    "###We are going to sample one from each of these actions independently and then we are just going to select greedily\n",
    "###This means that if you very uncertain that their is a big possibility that you are really going to select something that is pretty high, you are going to sample a candidate true value that is really high\n",
    "###There is also big probability that you are going to select sth that is really low not everytime but you'll select one more often\n",
    "###So you're uncertainity must be really big for you to be selected or you're current mean value must be very big(Those are 2 reasons why you might be selected)\n",
    "\n",
    "###Value of information\n",
    "###Exploration is valuable because information is valuable\n",
    "###Can we quantify the value of information\n",
    "###You gain more information when you are uncertain \n",
    "###Therefore it makes sense to explore novel situations more\n",
    "###If we know value of information we can trade off exploration and exploitation\n",
    "\n",
    "###Information state space\n",
    "###We have viewed bandits as one step decision making problems \n",
    "###Can also view as sequential decision-making problems \n",
    "###At each step there is an information state s(prime) summarising all information accumulated so far\n",
    "###Each action a causes a transtion to a new information state s(prime) (by adding information), with probability p(s prime | a,s prime)\n",
    "###We then have a markov decision problem\n",
    "###Here states = observations = internal information state\n",
    "###Even in bandits, actions affect the future after all, because they affect learning\n",
    "\n",
    "###Solving information state space Bandits \n",
    "###We formulated the bandit as an infinte MDP over information states \n",
    "###This can be solved by reinforcement learning \n",
    "###Model free reinforcement learning\n",
    "###-e.g Q-learning(Duff,1994)\n",
    "###Bayesian model based reinforcement learning \n",
    "###-e.g-Gittins indices (Gittins,1979)\n",
    "###The latter approach is known as Bayes adaptive RL\n",
    "###Finds Bayes optimal exploration/exploitation trade off with respect to the prior distribution\n",
    "###Can be unwieldy...unclear how to scale\n",
    "\n",
    "###Policy search \n",
    "###What is the idea? we want to learn a policy directly,we are not going to learn a value function necessarily,we are not going to learn a model,\n",
    "###we are just going to parameterize a policy and then try to learn that.How can you do that?\n",
    "###for instance we can define a probability distribution on the action(soft max)-it just means that your probability of selecting an action is now proportional\n",
    "###to the exponentiated preference(H)\n",
    "\n",
    "###Policy gradients\n",
    "###Idea: update policy parameters such that the expected value increases\n",
    "###We can consider *gradient ascent* on the expected value\n",
    "###....\n",
    "###Can we compute this gradient?\n",
    "\n",
    "\n",
    "\n",
    "###Reinforcement learnng lesson 3\n",
    "###Last lecture: multiple actions,but only one state -no model\n",
    "###This lecture:\n",
    "###-Formalize the problem with full sequential structure\n",
    "###-Discuss first class of solution methods which assume true model is given\n",
    "###-These methods are called dynamic programming\n",
    "###Next lectures: use similar ideas, but use sampling instead of true model\n",
    "\n",
    "###Formalizing the RL interface\n",
    "###We discuss a mathematical formulation of the agent-environment interaction\n",
    "###This is called a Markov decision process\n",
    "###We can then talk clearly about the objective and how to reach it\n",
    "###you can clearly see the environment state \n",
    "\n",
    "###Introduction to MDPs\n",
    "###Markov decision processes (MDPs) formally describe an environment\n",
    "###For now,assume the environment is fully observable:\n",
    "###the current observation contains all relevant information \n",
    "###Almost all RL problms can be formalized as MDPs, e,g.,\n",
    "###-Optimal control primarily deals with continuous MDPs\n",
    "###-Partially observable problems can be converted into MDPs\n",
    "###-Bandits are MDPs with one state \n",
    "\n",
    "### A process is Markov -given the future is independent of the past given the present\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
